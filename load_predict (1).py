# -*- coding: utf-8 -*-
"""load_predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KXIAZMZQuT3Zawpui57z6am56poojky2
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

import json
from collections import Counter
import re
import zipfile

path = "/content/drive/MyDrive/News_Category_Dataset_v3.json"

"""**DIRECT LINK TO DATASET**

https://www.kaggle.com/datasets/rmisra/news-category-dataset
"""

data = pd.read_json(path , lines = True)

data.head()

data.info()

data["category"][0]

values = data['category'].unique()

values

values = ['U.S. NEWS', 'COMEDY', 'PARENTING', 'WORLD NEWS', 'CULTURE & ARTS',
       'TECH', 'SPORTS', 'ENTERTAINMENT', 'POLITICS', 'WEIRD NEWS',
       'ENVIRONMENT', 'EDUCATION', 'CRIME', 'SCIENCE', 'WELLNESS',
       'BUSINESS', 'STYLE & BEAUTY', 'FOOD & DRINK', 'MEDIA',
       'QUEER VOICES', 'HOME & LIVING', 'WOMEN', 'BLACK VOICES', 'TRAVEL',
       'MONEY', 'RELIGION', 'LATINO VOICES', 'IMPACT', 'WEDDINGS',
       'COLLEGE', 'PARENTS', 'ARTS & CULTURE', 'STYLE', 'GREEN', 'TASTE',
       'HEALTHY LIVING', 'THE WORLDPOST', 'GOOD NEWS', 'WORLDPOST',
       'FIFTY', 'ARTS', 'DIVORCE']

len(values)

original_value_counts = data.category.value_counts()

original_value_counts.unique()

import matplotlib.pyplot as plt
plt.figure(figsize=(20,15))
plt.tick_params(axis= "x",labelrotation=70)
fig = plt.bar(values, original_value_counts)
plt.show()

"""As we don't have information category so we replaced all of our categories into "information"
"""

values = ['U.S. NEWS', 'COMEDY', 'PARENTING', 'WORLD NEWS', 'CULTURE & ARTS',
       'TECH', 'SPORTS', 'POLITICS', 'WEIRD NEWS',
       'ENVIRONMENT', 'EDUCATION', 'CRIME', 'SCIENCE', 'WELLNESS',
       'BUSINESS', 'STYLE & BEAUTY', 'FOOD & DRINK', 'MEDIA',
       'QUEER VOICES', 'HOME & LIVING', 'WOMEN', 'BLACK VOICES', 'TRAVEL',
       'MONEY', 'RELIGION', 'LATINO VOICES', 'IMPACT', 'WEDDINGS',
       'COLLEGE', 'PARENTS', 'ARTS & CULTURE', 'STYLE', 'GREEN', 'TASTE',
       'HEALTHY LIVING', 'THE WORLDPOST', 'GOOD NEWS', 'WORLDPOST',
       'FIFTY', 'ARTS', 'DIVORCE']

data["category"] = data["category"].replace(values, "information")

updated_labels = data["category"].value_counts()

updated_labels

plt.bar(["information","ENTERTAINMENT"],[192165,17362])

"""AS we can see our dataset is haeavily imbalanced after replacement, we will balance is using technique called undersampling."""

data.head()

"""Importing under sampling module and collection module for balancing the dataset """

from imblearn.under_sampling import RandomUnderSampler
from collections import Counter

x = data[["headline" , "short_description"]]
y = data[["category"]]

x["text"] =  x["headline"] + " "+ x["short_description"]

x.head()

print(Counter(y))

ros = RandomUnderSampler(sampling_strategy="majority")
x_ros, y_ros = ros.fit_resample(x,y)

print(Counter(y_ros))

y.value_counts()

"""It appears that the data has been resampled to align with the minority data points."""

y_ros.value_counts()

"""The code is selecting the "text" column from the "x_ros" dataframe and assigning the resulting subset back to "x_ros". We wil use x_ros for data pre-processing"""

x_ros = x_ros[["text"]]
x_ros.head()

y_ros.head()

data.value_counts('category')

"""**DATA PRE-PROCESSING**

"""

import nltk
from nltk.stem import WordNetLemmatizer
import string
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import word_tokenize

"""---------------------------------------------------------------------------
The code defines two functions for text preprocessing. The black_txt(token) function returns True if the given token is not a stopword, not a punctuation mark, and has a length greater than two, otherwise it returns False.

The clean_txt(sentence) function takes a sentence as input and performs the following operations:

It removes any apostrophes present in the sentence using re.sub().

It removes any non-alphanumeric characters or digits present in the sentence using re.sub().

It converts the sentence to lowercase and tokenizes it using word_tokenize() function from the NLTK library.

It applies lemmatization to each token using the WordNetLemmatizer() function from the NLTK library.

It filters out unwanted tokens using the black_txt(token) function defined earlier.


It joins the remaining tokens back into a string using the join() function and returns the preprocessed sentence.







"""

stop_words_ = set(stopwords.words('english'))
wn = WordNetLemmatizer()


def black_txt(token):
    return  token not in stop_words_ and token not in list(string.punctuation)  and len(token)>2
  
  
def clean_txt(sentence):
  clean_text = []
  clean_text2 = []
  text = re.sub("'", "",sentence)
  text=re.sub("(\\d|\\W)+"," ",sentence)    
  clean_text = [ wn.lemmatize(word, pos="v") for word in word_tokenize(text.lower()) if black_txt(word)]
  clean_text2 = [word for word in clean_text if black_txt(word)]
  return " ".join(clean_text2)

print(type(x_ros))

# this will be our cleaned data
x_ros["cleaned_text"] = x_ros["text"].apply(clean_txt)

x_ros["cleaned_text"]

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_ros, y_ros, test_size=0.2, random_state=42)

x_train.shape , y_train.shape , x_test.shape , y_test.shape

x_train = x_train[["cleaned_text"]]
x_train.head()
x_train.shape

x_test = x_test[["cleaned_text"]]
x_test.head()
x_test.shape

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

y_ros

"""Using tf-idf for feature extraction."""

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
x_train_vec = vectorizer.fit_transform(x_train["cleaned_text"])
x_test_vec = vectorizer.transform(x_test["cleaned_text"])

x_train_vec.shape

"""To train a machine learning model for natural language processing, we will utilize the support vector machine (SVM) and naive Bayes algorithms, which are widely recognized as effective methods in this field and have been shown to yield excellent results."""

from sklearn import svm
clf = svm.SVC()
clf.fit(x_train_vec, y_train_encoded)

y_pred = clf.predict(x_test_vec)
y_pred

y_pred =np.expand_dims(y_pred , axis=0)

y_pred.shape

y_pred

y_test_encoded

y_test_encoded = np.expand_dims(y_test_encoded, axis=(0))

#y_pred.shape
y_test_encoded.shape

def accuracy_score(y_pred , y_true):

  count = 0
  for i in range(len(y_pred[0])):
    if y_true[0][i] == y_pred[0][i]:
      count = count + 1
    else:
      pass
  return(count/len(y_pred[0])* 100)

accuracy_score(y_pred , y_test_encoded)

"""As shown, we got a accuracy of 88.86 using svm that is pretty good. We will check if naive bayes can give better accuracy."""

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

nb.fit(x_train_vec, y_train_encoded)
y_pred_nb = nb.predict(x_test_vec)

y_pred_nb

y_pred_nb = np.expand_dims(y_pred_nb, axis=0)

y_pred_nb.shape

y_test_encoded.shape

"""Predicting new article category after training our dataset. """

new_article = "This is a new article about sports."
new_article_features = vectorizer.transform([new_article])
prediction = nb.predict(new_article_features)[0]
print(f"Prediction: {prediction}")

accuracy_score(y_pred_nb , y_test_encoded)



